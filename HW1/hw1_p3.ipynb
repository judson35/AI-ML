{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Programming Problem 3 (10 points)\n",
    "\n",
    "In this problem, you will implement a K-NN regressor from scratch. Start by running the following cell to load the dataset.\n",
    "\n",
    "Dataset:\n",
    "- `w1_data`: $w_1$ values\n",
    "- `w2_data`: $w_2$ values\n",
    "- `L_data`: $L$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 876\n",
    "w1_data = np.random.uniform(-1,1,N)\n",
    "w2_data = np.random.uniform(-1,1,N)\n",
    "L_data = np.cos(4*w1_data + w2_data/4 - 1) + w2_data**2 + 2*w1_data**2\n",
    "\n",
    "plt.figure(figsize=(5,4.2),dpi=120)\n",
    "plt.scatter(w1_data,w2_data,s=10,c=L_data,cmap=\"jet\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.title(\"Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K - Nearest Neighbors Regressor\n",
    "\n",
    "### Distance function\n",
    "Now we will implement an unweighted K-NN regressor. First, finish the function `distance(w1, w2)` which computes the euclidean distance between a point `[w1, w2]` and each pair from `w1_data, w2_data`.\n",
    "The function should return an array of distances with the same length as `w1_data` or `w2_data`. Instead of using a for loop, you can do this by subtracting each individual scalar from the corresponding data array. For example, `w1 - w1_data` is an array that contains the difference between `w1` and each element in `w1_data`.\n",
    "Complete the function to compute the array $\\sqrt{(w_1 - w_{1,data (i)})^2 + (w_2 - w_{2,data (i)})^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(w1, w2):\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "# Check that the function outputs the correct array size\n",
    "assert(distance(0, 0).shape == w1_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting a distance array\n",
    "\n",
    "You can get the k-smallest elements of an array by using the `np.argpartition()` function. `np.argpartition(A, k)[:k]` returns an array of `k` indices corresponding to the k-smallest values in `A`.\n",
    "If we apply this to an array of distances from a point $w$ to each data point, we can get the indices of the k-nearest neighbors of $w$. Complete the function below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_indices(w1, w2, k):\n",
    "    d = distance(w1, w2)\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "# Check the function on the point w=(0,0) with k=5\n",
    "indices = get_knn_indices(0,0,5)\n",
    "print(\"5 points nearest (0,0):\", indices, \"\\n(Should be 255 733 538 815 501)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unweighted regression\n",
    "\n",
    "After acquiring the indices of the nearest points, you can determine the output values at these points by indexing into `L_data`, as in: `L_data[indices]`. Then, the function `np.mean()` can be used to compute the average value of these points. Complete the function below to do this. Return from this function a single value, the average of the k points closest to $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regress(w1, w2, k):\n",
    "    indices = get_knn_indices(w1, w2, k)\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "# Check the function on the point w=(0,0) with k=2\n",
    "val = knn_regress(0,0,2)\n",
    "print(\"Mean of 2 points nearest (0,0):\", val, \"\\n(Should be about 0.72)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the K-NN function\n",
    "\n",
    "Now we will evaluate the function on a meshgrid of points. `np.meshgrid` is used frequently for 2D visualization, so step through the next few cells to see how it works.\n",
    "\n",
    "First, we choose arrays of values for `w1` and `w2` that we want to be the x- and y- coordinates of grid points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_vals = np.linspace(-1,1,50)\n",
    "w2_vals = np.linspace(-1,1,50)\n",
    "print(\"w1 grid values:\",w1_vals)\n",
    "print(\"w2 grid values:\",w2_vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a 'cartesian product' of these arrays -- we get every combination of them; these will be our grid points. For this we use `np.meshgrid()`.\n",
    "\n",
    "Note that we flatten these arrays to get 1-D arrays of the grid points' coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1s, w2s = np.meshgrid(w1_vals, w2_vals)\n",
    "print(\"Size of w1 grid point array:\", w1s.shape)\n",
    "print(\"Size of w2 grid point array:\", w2s.shape)\n",
    "\n",
    "w1_grid, w2_grid = w1s.flatten(), w2s.flatten()\n",
    "print(\"Flattened size of w1 grid point array:\", w1_grid.shape)\n",
    "print(\"Flattened size of w2 grid point array:\", w2_grid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can loop through these arrays to call our K-NN regression function on the whole meshgrid, and plot it. Here we set k = 4, but this will be changed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "L_grid = np.zeros_like(w1_grid)\n",
    "for i in range(len(L_grid)):\n",
    "    L_grid[i] = knn_regress(w1_grid[i], w2_grid[i],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4.2),dpi=120)\n",
    "plt.scatter(w1_grid,w2_grid,s=10,c=L_grid,cmap=\"jet\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.title(\"K-NN Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Go back a couple cells and experiment with changing the `k` value. Is the regression function \"smoother\" with lower or higher `k`? Why do you think that is?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
