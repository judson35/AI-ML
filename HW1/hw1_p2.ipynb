{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Programming Problem 2 (10 points)\n",
    "\n",
    "In this problem, we are given a function $L(w_1, w_2)$ with a known functional form. You will perform gradient descent to find a global minimum. The goal is to find what initial guesses and learning rates (step sizes) lead the algorithm to find the global minimum.\n",
    "\n",
    "The function $L(w_1, w_2)$ is defined as:\n",
    "$$ L(w_1, w_2) = \\cos(4 w_1 + w_2 / 4 - 1) + w_2^2 + 2 w_1^2 $$\n",
    "A Python function for `L(w_1, w_2)` is given.\n",
    "\n",
    "## Gradients\n",
    "First, we must define a gradient of $L$. That is $\\nabla L = \\left[ \\frac{\\partial L}{\\partial w_1} , \\frac{\\partial L}{\\partial w_2}\\right]$. First, compute these derivatives by hand. Then, in the cell below, complete the functions for the derivatives of `L` with respect to `w1` and `w2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def L(w1, w2):\n",
    "    return np.cos(4*w1 + w2/4 - 1) + w2*w2 + 2*w1*w1\n",
    "\n",
    "def dLdw1(w1, w2):\n",
    "    d = -4*np.sin(4*w1 + w2/4 - 1) + 4*w1\n",
    "    return d\n",
    "    \n",
    "\n",
    "def dLdw2(w1, w2):\n",
    "    d = -np.sin(4*w1 + w2/4 - 1)/4 + 2*w2\n",
    "    return d\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The function `plot_gd` performs gradient descent by calling your derivative functions. Take a look at how this works. Then, run the interactive gradient descent cell that follows and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gd(w1, w2, log_stepsize, log_steps):\n",
    "    stepsize = 10**log_stepsize\n",
    "    steps = int(10**log_steps)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    w1s = np.zeros(steps+1)\n",
    "    w2s = np.zeros(steps+1)\n",
    "\n",
    "    for i in range(steps):\n",
    "        w1s[i], w2s[i] = w1, w2\n",
    "        w1 = w1 - stepsize * dLdw1(w1s[i],w2s[i])\n",
    "        w2 = w2 - stepsize * dLdw2(w1s[i],w2s[i])\n",
    "    w1s[steps], w2s[steps] = w1, w2\n",
    "\n",
    "    # Plotting\n",
    "    vals = np.linspace(-1,1,50)\n",
    "    x, y = np.meshgrid(vals,vals)\n",
    "    z = L(x,y)\n",
    "\n",
    "    plt.figure(figsize=(7,5.8),dpi=120)\n",
    "    plt.contour(x,y,z,colors=\"black\", levels=np.linspace(-.5,3,6))\n",
    "    plt.pcolormesh(x,y,z,shading=\"nearest\",cmap=\"Blues\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.plot(w1s,w2s,\"g-\",marker=\".\",markerfacecolor=\"black\",markeredgecolor=\"None\")\n",
    "    plt.scatter(w1s[0],w2s[0],zorder=100, color=\"blue\",marker=\"o\",label=f\"$w_0$ = [{w1s[0]:.1f}, {w2s[0]:.1f}]\")\n",
    "    plt.scatter(w1,w2,zorder=100,color=\"red\",marker=\"x\",label=f\"$w^*$ = [{w1:.2f}, {w2:.2f}]\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.axis(\"equal\")\n",
    "    plt.box(False)\n",
    "    plt.xlabel(\"$w_1$\")\n",
    "    plt.ylabel(\"$w_2$\")\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title(f\"Step size = {stepsize:.0e}; {steps} steps\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeef6ce6fcc4bc0aee952375ec17edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w1 guess', layout=Layout(width='550px'), max=1.0, miâ€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout, FloatSlider, Dropdown\n",
    "\n",
    "slider1 = FloatSlider(\n",
    "    value=0,\n",
    "    min=-1,\n",
    "    max=1,\n",
    "    step=.1,\n",
    "    description='w1 guess',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=False,\n",
    "    layout = Layout(width='550px')\n",
    ")\n",
    "\n",
    "slider2 = FloatSlider(\n",
    "    value=0,\n",
    "    min=-1,\n",
    "    max=1,\n",
    "    step=.1,\n",
    "    description='w2 guess',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=False,\n",
    "    layout = Layout(width='550px')\n",
    ")\n",
    "\n",
    "slider3 = FloatSlider(\n",
    "    value=-1.5,\n",
    "    min=-3,\n",
    "    max=0,\n",
    "    step=.5,\n",
    "    description='step size',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=False,\n",
    "    layout = Layout(width='550px')\n",
    ")\n",
    "\n",
    "slider4 = FloatSlider(\n",
    "    value=2,\n",
    "    min=0,\n",
    "    max=3,\n",
    "    step=.25,\n",
    "    description='steps',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=False,\n",
    "    layout = Layout(width='550px')\n",
    ")\n",
    "\n",
    "\n",
    "interactive_plot = interactive(\n",
    "    plot_gd,\n",
    "    w1 = slider1,\n",
    "    w2 = slider2,\n",
    "    log_stepsize = slider3,\n",
    "    log_steps = slider4,\n",
    "    )\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '620px'\n",
    "\n",
    "interactive_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Play around with the sliders above to get an intuition for which initial conditions/learning rates lead us to find the global minimum at [-0.42, -0.05]. Then answer the following questions:\n",
    "\n",
    "1. Set $w_0$ to [0.2, 0.8] and step size to 1e-01. After 100 steps of gradient descent, what $w^*$ do we reach?\n",
    "\n",
    "[-0.42, -0.05]\n",
    "\n",
    "2. Keep parameters from the previous question, but change the initial guess to [0.3, 0.8]. Now what is the optimum we find?\n",
    "\n",
    "[0.8, 0.1]\n",
    "\n",
    "3. Set $w_0$ to [-1.0, -1.0] and number of iterations to 1000 and step size to 1e-03. What $w^*$ do we reach, and why is it not exactly the global minimum?\n",
    "\n",
    "[-0.42, -0.18]. We don't reach the global minimum because the step size is too small for how far away we started from the minimum. If the initial guess was closer or the step size was larger then we would've converged to the tru global minimum.\n",
    "\n",
    "4. In general, what happens if we set learning rate too large?\n",
    "\n",
    "If the learning rate is too large then we will never converge on a minimum because it will keep overshooting the local/global minimum that it is approaching.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
