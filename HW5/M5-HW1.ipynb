{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 (30 points)\n",
    "\n",
    "## Problem Description\n",
    "In this problem you will train decision tree and random forest models using sklearn on a real world dataset. The dataset is the *Cylinder Bands Data Set* from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Cylinder+Bands](https://archive.ics.uci.edu/ml/datasets/Cylinder+Bands). The dataset is generated from rotogravure printers, with 39 unique features, and a binary classification label for each sample. The class is either 0, for 'band' or 1 for 'no band', where banding is an undesirable process delay that arises during the rotogravure printing process. By training ML models on this dataset, you could help identify or predict cases where these process delays are avoidable, thereby improving the efficiency of the printing. For the sake of this exercise, we only consider features 21-39 in the above link, and have removed any samples with missing values in that range. No further processing of the data is required on your behalf. The data has been partitioned into a training and testing set using an 80/20 split. Your models will be trained on just the test set, and accuracy results will be reported on both the training and testing sets.\n",
    "\n",
    "Fill out the notebook as instructed, making the requested plots and printing necessary values. \n",
    "\n",
    "*You are welcome to use any of the code provided in the lecture activities.*\n",
    "\n",
    "#### Summary of deliverables:\n",
    "\n",
    "- Accuracy function\n",
    "- Report accuracy of the DT model on the training and testing set\n",
    "- Report accuracy of the Random Forest model on the training and testing set\n",
    "\n",
    "#### Imports and Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Use the `np.load()` function to load \"w5-hw1-train.npy\" (training data) and \"w5-hw1-test.npy\" (testing data). The first 19 columns of each are the features. The last column is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims: (291, 19)\n",
      "y_train dims: (291,)\n",
      "X_test dims: (73, 19)\n",
      "y_test dims: (73,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(\"data/w5-hw1-train.npy\")\n",
    "test_data = np.load(\"data/w5-hw1-test.npy\")\n",
    "X_train = train_data[:,0:19]\n",
    "y_train = train_data[:,-1]\n",
    "X_test = test_data[:,0:19]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "print(f\"X_train dims: {X_train.shape}\")\n",
    "print(f\"y_train dims: {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test dims: {X_test.shape}\")\n",
    "print(f\"y_test dims: {y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write an accuracy function\n",
    "\n",
    "Write a function `accuracy(pred,label)` that takes in the models prediction, and returns the percentage of predictions that match the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,label):\n",
    "    return 100*(np.sum(pred == label)/len(label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree model\n",
    "\n",
    "Train a decision tree using `DecisionTreeClassifier()` with a `max_depth` of 10 and using a `random_state` of 0 to ensure repeatable results. Print the accuracy of the model on both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 93.12714776632302%\n",
      "Test accuracy: 65.75342465753424%\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10,random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy(dt.predict(X_train),y_train)}%\")\n",
    "print(f\"Test accuracy: {accuracy(dt.predict(X_test),y_test)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a random forest model\n",
    "\n",
    "Train a random forest model using `RandomForestClassifier()` with a `max_depth` of 10, a `n_estimators` of 100, and using a random state of `0` to ensure repeatable results. Print the accuracy of the model on both the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.0%\n",
      "Test accuracy: 82.1917808219178%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=10, n_estimators=100,random_state=0)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy(rf.predict(X_train),y_train)}%\")\n",
    "print(f\"Test accuracy: {accuracy(rf.predict(X_test),y_test)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the performance of the models\n",
    "\n",
    "Compare the training and testing accuracy of the two models, and explain why the random forest model is advantageous compared to a standard decision tree model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy is much higher for both the decision tree classifier and the random forest classifier but the difference is slightly larger for the decision tree classifier. The overall performance in both training and test data predictions is better for the random forest classifier, however, because it isn't overfitting to the training data. Since the decision tree classifier is only trained on one set of data, the model isn't going to be as applicable to new data coming in whereas the random forest classifier is able to see many different combinations of features and data points making it much more generalizable than the normal decision tree classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
