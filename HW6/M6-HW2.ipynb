{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7 (30 Points)\n",
    "\n",
    "Data-driven field prediction models can be used as a substitute for performing expensive calculations/simulations in design loops. For example, after being trained on finite element solutions for many parts, they can be used to predict nodal von Mises stress for a new part by taking in a mesh representation of the part geometry. \n",
    "\n",
    "Consider the plane-strain compression problem shown in \"data/plane-strain.png\".\n",
    "\n",
    "In this problem you are given node features for 100 parts. These node features have been extracted by processing each part shape using a neural network. You will perform feature selection to determine which of these features are most relevant using feature selection tools in sklearn.\n",
    "\n",
    "*You are welcome to use any of the code provided in the lecture activities.*\n",
    "\n",
    "#### Summary of deliverables:\n",
    "SciKit-Learn Models: Print Train and Test MSE\n",
    "- `LinearRegression()` with all features\n",
    "- `DecisionTreeRegressor()` with all features\n",
    "- `LinearRegression()` with features selected by `RFE()`\n",
    "- `DecisionTreeRegressor()` with features selected by `RFE()`\n",
    "\n",
    "Feature Importance/Coefficient Visualizations\n",
    "- Feature importance plot for Decision Tree using all features\n",
    "- Feature coefficient plot for Linear Regression using all features\n",
    "- Feature importance plot for DT showing which features RFE selected\n",
    "- Feature coefficient plot for LR showing which features RFE selected\n",
    "\n",
    "Stress Field Visualizations: Ground Truth vs. Prediction\n",
    "- Test dataset shape index 8 for decision tree and linear regression with all features\n",
    "- Test dataset shape index 16 for decision tree and linear regression with RFE features\n",
    "\n",
    "Questions\n",
    "- Respond to the 5 prompts at the end\n",
    "\n",
    "#### Imports and Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def plot_shape(dataset, index, model=None, lims=None):\n",
    "    x = dataset[\"coordinates\"][index][:,0]\n",
    "    y = dataset[\"coordinates\"][index][:,1]\n",
    "\n",
    "    if model is None:\n",
    "        c = dataset[\"stress\"][index]\n",
    "    else:\n",
    "        c = model.predict(dataset[\"features\"][index])\n",
    "\n",
    "    if lims is None:\n",
    "        lims = [min(c),max(c)]    \n",
    "\n",
    "    plt.scatter(x,y,s=5,c=c,cmap=\"jet\",vmin=lims[0],vmax=lims[1])\n",
    "    plt.colorbar(orientation=\"horizontal\", shrink=.75, pad=0,ticks=lims)\n",
    "    plt.axis(\"off\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "def plot_shape_comparison(dataset, index, model, title=\"\"):\n",
    "    plt.figure(figsize=[6,3.2], dpi=120)\n",
    "    plt.subplot(1,2,1)\n",
    "    plot_shape(dataset,index)\n",
    "    plt.title(\"Ground Truth\",fontsize=9,y=.96)\n",
    "    plt.subplot(1,2,2)\n",
    "    c = dataset[\"stress\"][index]\n",
    "    plot_shape(dataset, index, model, lims = [min(c), max(c)])\n",
    "    plt.title(\"Prediction\",fontsize=9,y=.96)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = np.load(path)\n",
    "    coordinates = []\n",
    "    features = []\n",
    "    stress = []\n",
    "    N = np.max(dataset[:,0].astype(int)) + 1\n",
    "    split = int(N*.8)\n",
    "    for i in range(N):\n",
    "        idx = dataset[:,0].astype(int) == i\n",
    "        data = dataset[idx,:]\n",
    "        coordinates.append(data[:,1:3])\n",
    "        features.append(data[:,3:-1])\n",
    "        stress.append(data[:,-1])\n",
    "    dataset_train = dict(coordinates=coordinates[:split], features=features[:split], stress=stress[:split])\n",
    "    dataset_test = dict(coordinates=coordinates[split:], features=features[split:], stress=stress[split:])\n",
    "    X_train, X_test = np.concatenate(features[:split], axis=0), np.concatenate(features[split:], axis=0)\n",
    "    y_train, y_test = np.concatenate(stress[:split], axis=0), np.concatenate(stress[split:], axis=0)\n",
    "    return dataset_train, dataset_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_shape(dataset,index):\n",
    "    X = dataset[\"features\"][index]\n",
    "    y = dataset[\"stress\"][index]\n",
    "    return X, y\n",
    "\n",
    "def plot_importances(model, selected = None, coef=False, title=\"\"):\n",
    "    plt.figure(figsize=(6,2),dpi=150)\n",
    "    y = model.coef_ if coef else model.feature_importances_\n",
    "    N = 1+len(y)\n",
    "    x = np.arange(1,N)\n",
    "\n",
    "    plt.bar(x,y)\n",
    "\n",
    "    if selected is not None:\n",
    "        plt.bar(x[selected],y[selected],color=\"red\",label=\"Selected Features\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.xlabel(\"Feature\")\n",
    "\n",
    "    plt.ylabel(\"Coefficient\" if coef else \"Importance\")\n",
    "    plt.xlim(0,N)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "First, complete the code below to load the data and plot the von Mises stress fields for a few shapes.  \n",
    "You'll need to input the path of the data file, the rest is done for you. \n",
    "\n",
    "All training node features and outputs are in `X_train` and `y_train`, respectively. Testing nodes are in `X_test`, `y_test`.  \n",
    "\n",
    "`dataset_train` and `dataset_test` contain more detailed information such as node coordinates, and they are separated by shape.  \n",
    "Get features and outputs for a shape by calling `get_shape(dataset,index)`. `N_train` and `N_test` are the number of training and testing shapes in each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Define data_path\n",
    "\n",
    "dataset_train, dataset_test, X_train, X_test, y_train, y_test = load_dataset(data_path)\n",
    "N_train = len(dataset_train[\"stress\"])\n",
    "N_test = len(dataset_test[\"stress\"])\n",
    "\n",
    "plt.figure(figsize=[15,3.2], dpi=150)\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plot_shape(dataset_train,i)\n",
    "    plt.title(f\"Shape {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models with all features \n",
    "Create two models to fit the training data `X_train`, `y_train`:\n",
    "1. A `LinearRegression()` model\n",
    "2. A `DecisionTreeRegressor()` model with a `max_depth` of 20\n",
    "\n",
    "Print the training and testing MSE for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Use the `plot_shape_comparison()` function to plot the index 8 shape results in `dataset_test` for each model.\n",
    "\n",
    "Include titles to indicate which plot is which, using the `title` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 8\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "For a tree methods, \"feature importance\" can be computed, which can be done for an sklearn model using `.feature_importances_`.  \n",
    "\n",
    "Use the provided function `plot_importances()` to visualize which features are most important to the final decision tree prediction.  \n",
    "Then create another plot using the same function to visualize the linear regression coefficients by setting the \"coef\" argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection by Recursive Feature Elimination\n",
    "\n",
    "Using `RFE()` in sklearn, you can iteratively select a subset of only the most important features.  \n",
    "\n",
    "For both linear regression and decision tree (depth 20) models:\n",
    "1. Create a new model.\n",
    "2. Create an instance of `RFE()` with `n_features_to_select` set to 30.\n",
    "3. Fit the RFE model as you would a normal sklearn model.\n",
    "4. Report the train and test MSE.\n",
    "\n",
    "Note that the decision tree RFE model may take a few minutes to train.  \n",
    "Visit https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "Use the `plot_shape_comparison()` function to plot the index 16 shape results in `dataset_test` for each model.\n",
    "\n",
    "As before, include titles to indicate which plot is which, using the `title` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 16\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance with RFE\n",
    "Recreate the 2 feature importance/coefficent plots from earlier, but this time highlight which features were ultimately selected after performing RFE by coloring those features red. You can do this by setting the `selected` argument equal to an array of selected indices.  \n",
    "\n",
    "For an RFE model `rfe`, the selected feature indices can be obtained via `rfe.get_support(indices=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Did the MSE increase or decrease on test data for the Linear Regression model after performing RFE?  \n",
    "\n",
    "\n",
    "2. Did the MSE increase or decrease on test data for the Decision Tree model after performing RFE?  \n",
    "\n",
    "\n",
    "3. Describe the qualitative differences between the Linear Regression and the Decision Tree predictions.  \n",
    "\n",
    "\n",
    "4. Describe how the importance of features that were selected by RFE compare to that of features that were eliminated (for the decision tree).  \n",
    "\n",
    "\n",
    "5. Describe how the coefficients that were selected by RFE compare to that of features that were eliminated (for linear regression).  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
