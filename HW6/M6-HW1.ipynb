{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 (30 Points)\n",
    "\n",
    "During the lecture you worked with pipelines in SciKit-Learn to perform feature transformation before classification/regression using a pipeline. In this problem, you will look at another scaling method in a 2D regression context.\n",
    "\n",
    "*You are welcome to use any of the code provided in the lecture activities.*\n",
    "\n",
    "#### Summary of deliverables:\n",
    "Sklearn Models (no scaling): Print Train and Test MSE\n",
    "- Linear Regression (input degree 8 features)\n",
    "- SVR, C = 1000\n",
    "- KNN, K = 4\n",
    "- Random Forest, 100 estimators of max depth 10  \n",
    "\n",
    "Sklearn Pipeline (scaling + model): Print Train and Test MSE\n",
    "- Linear Regression (input degree 8 features)\n",
    "- SVR, C = 1000\n",
    "- KNN, K = 4\n",
    "- Random Forest, 100 estimators of max depth 10  \n",
    "\n",
    "Plots\n",
    "- 1x5 subplot showing model predictions on unscaled features, next to ground truth\n",
    "- 1x5 subplot showing pipeline predictions with features scaled, next to ground truth\n",
    "\n",
    "Questions\n",
    "- Respond to the prompts at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot(X, y, title=\"\"):\n",
    "    plt.scatter(X[:,0],X[:,1],c=y,cmap=\"jet\")\n",
    "    plt.colorbar(orientation=\"horizontal\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Complete the loading process below by inputting the path to the data file \"w6-p1-data.npy\"\n",
    "\n",
    "Training data is in `X_train` and `y_train`.\n",
    "Testing data is in `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Define path\n",
    "data = np.load(path)\n",
    "X, y = data[:,:2], data[:,2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=int(0.8*len(y)),random_state=0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plot(X_train, y_train, \"Training data\")\n",
    "plt.subplot(1,2,2)\n",
    "plot(X_test, y_test, \"Testing data\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models (no input scaling)\n",
    "Fit 4 models to the training data:\n",
    "- `LinearRegression()`. This should be a pipeline whose first step is `PolynomialFeatures()` with degree 7.\n",
    "- `SVR()` with C = 1000 and \"rbf\" kernel\n",
    "- `KNeighborsRegressor()` using 4 nearest neighbors\n",
    "- `RandomForestRegressor()` with 100 estimators of max depth 10\n",
    "\n",
    "Print the Train and Test MSE for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"LSR\", \"SVR\", \"KNN\", \"RF\"]\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the predictions\n",
    "Plot the predictions of each method on the testing data in a 1x5 subplot structure, with the ground truth values as the leftmost subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,4))\n",
    "plt.subplot(1,5,1)\n",
    "plot(X_test, y_test, \"GT Testing\")\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Scaling\n",
    "\n",
    "A `QuantileTransformer()` can transform the input data in a way that attempts to match a given distribution (uniform distribution by default).  \n",
    "- Create a quantile scaler with `n_quantiles = 800`.\n",
    "- Then, create a pipeline for each of the 4 types of models used earlier\n",
    "- Fit each pipeline to the training data, and again print the train and test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_names = [\"LSR, scaled\", \"SVR, scaled\", \"KNN, scaled\", \"RF, scaled\"]\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with scaled input\n",
    "As before, plot the predictions of each *scaled* method on the testing data in a 1x5 subplot structure, with the ground truth values as the leftmost subplot.  \n",
    "\n",
    "This time, for each plot, show the scaled data points instead of the original data. You can do this by calling `.transform()` on your quantile scaler. The scaled points should appear to follow a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Without transforming the input data, which model performed the best on test data? What about after scaling?  \n",
    "\n",
    "\n",
    "2. For each method, say whether scaling the input improved or worsened, how extreme the change was, and why you think this is.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
