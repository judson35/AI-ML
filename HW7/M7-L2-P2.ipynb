{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M7-L2 Problem 2\n",
    "\n",
    "Here you will create a simple neural network for regression in PyTorch. PyTorch will give you a lot more control and flexibility for neural networks than SciKit-Learn, but there are some extra steps to learn. \n",
    "\n",
    "Run the following cell to load our 1-D dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = np.array([0.        , 0.03448276, 0.06896552, 0.10344828, 0.13793103,0.17241379, 0.20689655, 0.24137931, 0.27586207, 0.31034483,0.34482759, 0.37931034, 0.4137931 , 0.44827586, 0.48275862,0.51724138, 0.55172414, 0.5862069 , 0.62068966, 0.65517241,0.68965517, 0.72413793, 0.75862069, 0.79310345, 0.82758621,0.86206897, 0.89655172, 0.93103448, 0.96551724, 1.        ]).reshape(-1,1)\n",
    "y = np.array([ 0.38914369,  0.40997345,  0.40282978,  0.38493705,  0.394214  ,0.41651437,  0.37573321,  0.39571087,  0.41265936,  0.41953955,0.50596807,  0.58059196,  0.6481607 ,  0.66050901,  0.67741369,0.67348567,  0.67696078,  0.63537378,  0.56446933,  0.48265412,0.39540671,  0.29878237,  0.15893846,  0.05525194, -0.10070259,-0.23055219, -0.35288448, -0.51317604, -0.63377544, -0.76849408]).reshape(-1,1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,4),dpi=250)\n",
    "plt.scatter(x,y,s=5,c=\"navy\",label=\"Data\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "PyTorch models only work with PyTorch Tensors, so we need to convert our dataset into a tensors.\n",
    "\n",
    "To convert these back to numpy arrays we can use:\n",
    "- `x.detach().numpy()`\n",
    "- `y.detach().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Module\n",
    "\n",
    "We create a subclass whose superclass is `nn.Module`, a basic predictive model, and we must define 2 methods.  \n",
    "\n",
    "**`nn.Module` subclass:**\n",
    "- `__init__()`\n",
    "  - runs when creating a new model instance\n",
    "  - includes the line `super().__init__()` to inherit parent methods from `nn.Module`\n",
    "  - sets up all necessary model components/parameters\n",
    "- `forward()`\n",
    "  - runs when calling a model instance\n",
    "  - performs a forward pass through the network given an input tensor.\n",
    "\n",
    "\n",
    "This class `Net_2_layer` is an MLP for regression with 2 layers. At initialization, the user inputs the number of hidden neurons per layer, the number of inputs and outputs, and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_2_layer(nn.Module):\n",
    "    def __init__(self, N_hidden=6, N_in=1, N_out=1, activation = F.relu):\n",
    "        super().__init__()\n",
    "        # Linear transformations -- these have weights and biases as trainable parameters, \n",
    "        # so we must create them here.\n",
    "        self.lin1 = nn.Linear(N_in, N_hidden)\n",
    "        self.lin2 = nn.Linear(N_hidden, N_hidden)\n",
    "        self.lin3 = nn.Linear(N_hidden, N_out)\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)  # Activation of first hidden layer\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)  # Activation at second hidden layer\n",
    "        x = self.lin3(x) # (No activation at last layer)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a model\n",
    "This model has 6 neurons at each hidden layer, and it uses ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_2_layer(N_hidden = 6, activation = F.relu)\n",
    "loss_curve = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters: Learning rate, number of epochs, loss function\n",
    "# (These can be tuned)\n",
    "lr = 0.005\n",
    "epochs = 1500\n",
    "loss_fcn = F.mse_loss\n",
    "\n",
    "# Set up optimizer to optimize the model's parameters using Adam with the selected learning rate\n",
    "opt = optim.Adam(params = model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    out = model(x) # Evaluate the model\n",
    "    loss = loss_fcn(out,y) # Calculate the loss -- error between network prediction and y\n",
    "\n",
    "    loss_curve.append(loss.item())\n",
    "\n",
    "    # Print loss progress info 25 times during training\n",
    "    if epoch % int(epochs / 25) == 0:\n",
    "        print(f\"Epoch {epoch} of {epochs}... \\tAverage loss: {loss.item()}\")\n",
    "\n",
    "    # Move the model parameters 1 step closer to their optima:\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.plot(loss_curve)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.linspace(0,1,100).reshape(-1,1)\n",
    "ys = model(xs)\n",
    "\n",
    "plt.figure(figsize=(5,4),dpi=250)\n",
    "plt.scatter(x,y,s=5,c=\"navy\",label=\"Data\")\n",
    "plt.plot(xs.detach().numpy(), ys.detach().numpy(),\"r-\",label=\"Prediction\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "In the cells below, create a new instance of `Net_2_layer`. This time, use 20 neurons per hidden layer, and an activation of `F.tanh`.\n",
    "Plot the loss curve and a visualization of the prediction with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
