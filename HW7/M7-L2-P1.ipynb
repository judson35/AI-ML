{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M7-L2 Problem 1\n",
    "\n",
    "In this function you will:\n",
    "- Learn to use SciKit-Learn's `MLPRegressor()` model\n",
    "- Look at the loss curve of an sklearn neural network\n",
    "- Try out multiple activation functions\n",
    "\n",
    "\n",
    "First, load the data in the following cell. This is the same data from M7-L1-P2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "x = np.array([0.        , 0.03448276, 0.06896552, 0.10344828, 0.13793103,0.17241379, 0.20689655, 0.24137931, 0.27586207, 0.31034483,0.34482759, 0.37931034, 0.4137931 , 0.44827586, 0.48275862,0.51724138, 0.55172414, 0.5862069 , 0.62068966, 0.65517241,0.68965517, 0.72413793, 0.75862069, 0.79310345, 0.82758621,0.86206897, 0.89655172, 0.93103448, 0.96551724, 1.        ]).reshape(-1,1)\n",
    "y = np.array([ 0.38914369,  0.40997345,  0.40282978,  0.38493705,  0.394214  ,0.41651437,  0.37573321,  0.39571087,  0.41265936,  0.41953955,0.50596807,  0.58059196,  0.6481607 ,  0.66050901,  0.67741369,0.67348567,  0.67696078,  0.63537378,  0.56446933,  0.48265412,0.39540671,  0.29878237,  0.15893846,  0.05525194, -0.10070259,-0.23055219, -0.35288448, -0.51317604, -0.63377544, -0.76849408])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,4),dpi=250)\n",
    "plt.scatter(x,y,s=5,c=\"navy\",label=\"Data\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MLPRegressor()`\n",
    "\n",
    "Here, we create a simple MLP Regressor in sklearn and plot the results. The model is created and fitted in the same way as any other sklearn model.\n",
    "We choose hidden layer sizes 10,10. Note that our input and output are both 1-D, but we don't need to specify this at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=[10,10]) # Tune here\n",
    "mlp.fit(x, y)\n",
    "\n",
    "xs = np.linspace(0,1)\n",
    "ys = mlp.predict(xs.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(5,4),dpi=250)\n",
    "plt.scatter(x,y,s=5,c=\"navy\",label=\"Data\")\n",
    "plt.plot(xs,ys,\"r-\",linewidth=1,label=\"MLP\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning training hyperparameters\n",
    "Chances are, the model above did a poor job fitting the data. Try changing the following parameters when initializing the `MLPRegressor` in the cell above:\n",
    "- `max_iter` (this will need to be very large)\n",
    "- `tol` (this will need to be very small)\n",
    "\n",
    "You can read about what these do at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "## Question\n",
    "1. What values of `max_iter` and `tol` gave you a reasonable fit?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve\n",
    "\n",
    "We can look at the loss curve by accessing `mlp.loss_curve_`. Let's plot this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mlp.loss_curve_\n",
    "plt.figure(dpi=250)\n",
    "plt.plot(loss,label=\"MLP\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "Sklearn provides the following activation functions:\n",
    "- `\"identity\"` (This is a linear function, it should not give good results)\n",
    "- `\"logistic\"` (We call this 'sigmoid', although both this and tanh are sigmoid functions)\n",
    "- `\"tanh\"`\n",
    "- `\"relu\"`\n",
    "\n",
    "Run the following cell to train a model on each. They can be accessed via, for example: `models[\"relu\"]` for the relu activation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "models = dict()\n",
    "\n",
    "for act in activations:\n",
    "    model = MLPRegressor([10,10],random_state=50, activation=act,max_iter=100000,tol=1e-11)\n",
    "    model.fit(x,y)\n",
    "    models[act] = model\n",
    "\n",
    "xs = np.linspace(0,1)\n",
    "plt.figure(figsize=(5,4),dpi=250)\n",
    "plt.scatter(x,y,s=5,c=\"navy\",label=\"Data\")\n",
    "\n",
    "for act in activations:\n",
    "    model = models[act]\n",
    "    ys = model.predict(xs.reshape(-1,1))\n",
    "    plt.plot(xs,ys,linewidth=1,label=act)\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss curves\n",
    "Now, create another loss curve plot, but this time, include all four MLP models with a legend indicating which activation function corresponds to each curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "\n",
    "2. Which activation functions produced a good fit?\n",
    "\n",
    "\n",
    "3. Which activation function's model converged the \"slowest\"?\n",
    "\n",
    "\n",
    "4. Of the networks that fit well, which activation function's model converged the \"fastest\"?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
