{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M7-L1 Problem 1\n",
    "\n",
    "In this problem, you will implement a perceptron function that can take in multiple inputs at once as a matrix and output the result of multiplying by a weight matrix and adding a bias vector.\n",
    "Then you will use this function in a loop to implement a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `perceptron_layer()`\n",
    "\n",
    "Complete the function definition for `perceptron_layer(x, weight, bias)`.\n",
    "Inputs:\n",
    "- `x`: An $N \\times n$ matrix of $N$ inputs, each with $n$ features.\n",
    "- `weight`: An $m \\times n$ weight matrix, to be multiplied by the input `x`\n",
    "- `bias`: A 1-D array of $m$ biases, to be added to the $m$ outputs  \n",
    "\n",
    "Return:\n",
    "- $N \\times m$ output $a$\n",
    "\n",
    "\n",
    "$a$ can be obtained by multiplying the weight matrix by the inputs, then adding bias. You must figure out how to make the dimensions work out (e.g. by transposing as necessary) to give the correct size result.\n",
    "\n",
    "A nonlinear activation would be applied after this function in the context of an MLP, so don't include it in the function. A test case is included for you to check for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your result\n",
      "[[ -4.5   0.5   4.5  -3. ]\n",
      " [-13.5   3.5   9.5  -3. ]\n",
      " [-22.5   6.5  14.5  -3. ]]\n",
      "Correct result:\n",
      "[[ -4.5   0.5   4.5  -3. ]\n",
      " [-13.5   3.5   9.5  -3. ]\n",
      " [-22.5   6.5  14.5  -3. ]]\n"
     ]
    }
   ],
   "source": [
    "def perceptron_layer(x, weight, bias):\n",
    "    return x@weight.T + bias\n",
    "\n",
    "# Example: N = 3, n = 2, m = 4\n",
    "x = np.array([[1,2],[3,4],[5,6]])\n",
    "weight = np.array([[-1.5, -3], [0.5, 1], [1, 1.5], [2, -2]])\n",
    "bias = np.array([3, -2, .5, -1])\n",
    "a = perceptron_layer(x, weight, bias)\n",
    "result = np.array(np.array([[ -4.5,   0.5,   4.5,  -3. ],[-13.5,   3.5,   9.5,  -3. ],[-22.5,   6.5,  14.5,  -3. ]]))\n",
    "\n",
    "print(\"Your result\", a, sep=\"\\n\")\n",
    "print(\"Correct result:\", result, sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `MLP()`\n",
    "\n",
    "Now by looping through several perceptron layers, you can create a multilayer perceptron (AKA a Neural Network!). Complete the function below to do this.\n",
    "Inputs: \n",
    "- `x`: An $N \\times n$ matrix of $N$ inputs, each with $n$ features.\n",
    "- `weights`: A list of weight matrices\n",
    "- `biases`: A list of bias vectors  \n",
    "\n",
    "Return:\n",
    "- Result of applying each perceptron layer with activation, to the input one-by-one\n",
    "\n",
    "Apply sigmoid activation (a sigmoid function is given) on all layers EXCEPT the final layer.\n",
    "\n",
    "A test case is provided for you to check your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def MLP(x, weights, biases):\n",
    "    a = x\n",
    "    for i,weight in enumerate(weights[:-1]):\n",
    "        a = sigmoid(perceptron_layer(a, weight, biases[i]))\n",
    "\n",
    "    return perceptron_layer(a, weights[-1], biases[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Your result: [[0.029 0.267 0.314 0.027 0.319 0.297 0.331 0.343 0.187 0.335]].T\n",
      "Correct result: [[0.029 0.267 0.314 0.027 0.319 0.297 0.331 0.343 0.187 0.335]].T\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "np.random.seed(0)\n",
    "dims = [2,6,8,3,1]\n",
    "weights = []\n",
    "biases = []\n",
    "for i,_ in enumerate(dims[:-1]):\n",
    "    weights.append(np.random.standard_normal([dims[i+1],dims[i]]))\n",
    "    biases.append(np.random.rand(dims[i+1]))\n",
    "x = np.random.uniform(-10,10,size=[10,2])\n",
    "\n",
    "result = np.array([[0.029],[0.267],[0.314],[0.027],[0.319],[0.297],[0.331],[0.343],[0.187],[0.335]])\n",
    "y = MLP(x, weights, biases)\n",
    "\n",
    "print(\"   Your result: \", y.T, \".T\",sep=\"\")\n",
    "print(\"Correct result: \", result.T, \".T\", sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
