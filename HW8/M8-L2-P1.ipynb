{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M8-L2 Problem 1\n",
    "\n",
    "In this problem, you will create 3 regression networks with different complexities in PyTorch. By looking at the validation loss curves superimposed on the training loss curves, you should determine which model is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "def generate_data():\n",
    "    np.random.seed(5)\n",
    "    N = 25\n",
    "    x = np.random.normal(np.linspace(0,1,N),0.01).reshape(-1,1)\n",
    "    y = np.random.normal(np.sin(5*(x+0.082)),0.2)\n",
    "    train_mask = np.zeros(N,dtype=np.bool_)\n",
    "    train_mask[np.random.permutation(N)[:int(N*0.8)]] = True\n",
    "    train_x, val_x = torch.Tensor(x[train_mask]), torch.Tensor(x[np.logical_not(train_mask)])\n",
    "    train_y, val_y = torch.Tensor(y[train_mask]), torch.Tensor(y[np.logical_not(train_mask)])\n",
    "    \n",
    "    return train_x, val_x, train_y, val_y\n",
    "\n",
    "def train(model, lr=0.0001, epochs=10000):\n",
    "    train_x, val_x, train_y, val_y = generate_data()\n",
    "    opt = optim.Adam(model.parameters(),lr=lr)\n",
    "    lossfun = nn.MSELoss()\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        loss_train = lossfun(train_y, model(train_x))\n",
    "        train_hist.append(loss_train.item())\n",
    "\n",
    "        model.eval()\n",
    "        loss_val = lossfun(val_y, model(val_x))\n",
    "        val_hist.append(loss_val.item())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_train.backward()\n",
    "        opt.step()\n",
    "\n",
    "    train_hist, val_hist = np.array(train_hist), np.array(val_hist)\n",
    "    return train_hist, val_hist\n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    plt.plot(train_loss,label=\"Training\")\n",
    "    plt.plot(val_loss,label=\"Validation\",linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "def plot_data(model = None):\n",
    "    train_x, val_x, train_y, val_y = generate_data()\n",
    "    plt.scatter(train_x, train_y,s=8,label=\"Train Data\")\n",
    "    plt.scatter(val_x, val_y,s=12,marker=\"x\",label=\"Validation Data\",linewidths=1)\n",
    "\n",
    "    if model is not None:\n",
    "        xvals = torch.linspace(0,1,1000).reshape(-1,1)\n",
    "        plt.plot(xvals.detach().numpy(),model(xvals).detach().numpy(),label=\"Model\",color=\"black\")\n",
    "    \n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "def get_loss(model):\n",
    "    lossfun = nn.MSELoss()\n",
    "    train_x, val_x, train_y, val_y = generate_data()\n",
    "    loss_train = lossfun(train_y, model(train_x))\n",
    "    loss_val = lossfun(val_y, model(val_x))\n",
    "    return loss_train.item(), loss_val.item()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,3),dpi=250)\n",
    "plot_data()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding neural networks for regression\n",
    "\n",
    "Here, create 3 neural networks from scratch. You can use `nn.Sequential()` to simplify things. Each network should have 1 input and 1 output. After each hidden layer, apply ReLU activation. Name the models `model1`, `model2`, and `model3`, with architectures as follows:\n",
    "\n",
    "- `model1`: 1 hidden layer with 4 neurons. That is, the network should have a linear transformation from size 1 to size 4. Then a ReLU activation should be applied. Finally, a linear transformation from size 4 to size 1 gives the network output. (Note: Your regression network should not have an activation after the last layer!)\n",
    "\n",
    "- `model2`: Hidden sizes (16, 16). (Two hidden layers, each with 16 neurons)\n",
    "\n",
    "- `model3`: Hidden sizes (128, 128, 128). (3 hidden layers, each with 128 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Loss curves\n",
    "\n",
    "The following cell calls the provided function `train` to train each of your neural network models. The training and validation curves are then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers=[\"(4)\",\"(16, 16)\",\"(128, 128, 128)\"]\n",
    "\n",
    "plt.figure(figsize=(15,3),dpi=250)\n",
    "for i,model in enumerate([model1, model2, model3]):\n",
    "    loss_train, loss_val = train(model)\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plot_loss(loss_train, loss_val)\n",
    "    plt.ylim(0,0.6)\n",
    "    plt.title(f\"Hidden layers: {hidden_layers[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "Let's print the values of MSE on the training and testing/validation data after training. Make note of which model is \"best\" (has lowest testing error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate([model1, model2, model3]):\n",
    "    train_loss, val_loss = get_loss(model)\n",
    "    print(f\"Model {i+1}, hidden layers {hidden_layers[i]:>15}:   Train MSE: {train_loss:.4f}    Test MSE: {val_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Now we can look at how good each model's predictions are. Run the following cell to generate a visualization plot, then answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3),dpi=250)\n",
    "for i,model in enumerate([model1, model2, model3]):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plot_data(model)\n",
    "    plt.title(f\"Hidden layers: {hidden_layers[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. For the model that overfits the most, describe what happens to the loss curves while training.  \n",
    "\n",
    "\n",
    "2. For the model that underfits the most, describe what happens to the loss curves while training.  \n",
    "\n",
    "\n",
    "3. For the \"best\" model, what happens to the loss curves while training?  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
