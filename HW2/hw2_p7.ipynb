{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Programming Problem 7 (30 points)\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "In this problem you will implement polynomial linear least squares regression on two datasets, with and without regularization. Additionally, you will use gradient descent to optimize for the model parameters.\n",
    "\n",
    "Fill out the notebook as instructed, making the requested plots and printing necessary values. \n",
    "\n",
    "*You are welcome to use any of the code provided in the lecture activities.*\n",
    "\n",
    "#### Summary of deliverables:\n",
    "\n",
    "Results:\n",
    "- Print fitted model parameters `w` for the 4 models requested without regularization\n",
    "- Print fitted model parameters `w` for the 2 models requested *with* $L_2$ regularization\n",
    "- Print fitted model parameters `w` for the one model solved via gradient descent\n",
    "\n",
    "Plots:\n",
    "- 2 plots of each dataset along with the ground truth function\n",
    "- 4 plots of the fitted function along with the respective data and ground truth function for LLS without regularization\n",
    "- 2 plots of the fitted function along with the respective data and ground truth function for LLS with $L_2$ regularization\n",
    "- 1 plot of the fitted function along with the respective data and ground truth function for LLS with $L_2$ regularization solved via gradient descent\n",
    "\n",
    "Discussion:\n",
    "- Discussion of challenges fitting complex models to small datasets\n",
    "- Discussion of difference between the $L_2$ regularized model versus the standard model\n",
    "- Dicussion of whether gradient descent could get stuck in a local minimum\n",
    "- Discussion of gradient descent results versus closed form results\n",
    "\n",
    "#### Imports and Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gt_function():\n",
    "    xt = np.linspace(0,1,101)\n",
    "    yt = np.sin(2 *np.pi*xt)\n",
    "    return xt, yt\n",
    "\n",
    "def plot_data(x,y,xt,yt,title = None):\n",
    "    # Provide title as a string e.g. 'string'\n",
    "    plt.plot(x,y,'bo',label = 'Data')\n",
    "    plt.plot(xt,yt,'g-', label = 'Ground Truth')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_model(x,y,xt,yt,xr,yr,title = None):\n",
    "    # Provide title as a string e.g. 'string'\n",
    "    plt.plot(x,y,'bo',label = 'Data')\n",
    "    plt.plot(xt,yt,'g-', label = 'Ground Truth')\n",
    "    plt.plot(xr,yr,'r-', label = 'Fitted Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize the data\n",
    "\n",
    "The data is contained in `d10.npy` and `d100.npy` and can be loaded with `np.load()`. \n",
    "\n",
    "Store the data as:\n",
    "- `x10` and `x100` (the first column of the data)\n",
    "- `y10` and `y100` (the second column of the data)\n",
    "\n",
    "Generate the ground truth function $f(x)=\\textrm{sin}(2\\pi x)$ using `xt, yt = gt_function()`.\n",
    "\n",
    "Then visualize the each dataset with `plotxy(x,y,xt,yt,title)` with an appropriate title. You should generate two plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement polynomial linear regression\n",
    "\n",
    "Now you will implement polynomial linear least squares regression without regularization using the closed form solution from lecture to compute the model parameters. You will consider the following 4 cases:\n",
    "\n",
    "1. Data:  data10.txt, Model: 2nd order polynomial (highest power of $x$ in your regression model = 2)  \n",
    "1. Data: data100.txt, Model: 2nd order polynomial (highest power of $x$ in your regression model = 2)  \n",
    "1. Data:  data10.txt, Model: 9th order polynomial (highest power of $x$ in your regression model = 9)  \n",
    "1. Data: data100.txt, Model: 9th order polynomial (highest power of $x$ in your regression model = 9)\n",
    "\n",
    "For each model:\n",
    "- Print the learned model parameters `w`\n",
    "- Use the model parameters `w` to predict `yr` values over a range of x values given by `xr = np.linspace(0,1,101)`\n",
    "- Plot the data, ground truth function, and regressed model using `plot_model(x,y,xt,yt,xr,yr,title)` with an appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "\n",
    "When the sample size (number of data points) is small, what issues or tendencies do you see with complex models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement polynomial linear regression with $L_2$ regularization\n",
    "\n",
    "You will repeat the previous section, but this time using $L_2$ regularization. Your regularization term should be $\\lambda w' \\mathbb{I}_m w$, where $\\lambda = e^{-10}$, and $\\mathbb{I}_m$ is the modified identity matrix that masks out the bias term  from  regularization. \n",
    "\n",
    "You will consider only two cases:\n",
    "1. Data:  data10.txt, Model: 9th order polynomial (highest power of $x$ in your regression model = 9)  \n",
    "1. Data: data100.txt, Model: 9th order polynomial (highest power of $x$ in your regression model = 9)  \n",
    "\n",
    "For each model:\n",
    "- Print the learned model parameters `w`\n",
    "- Use the model parameters `w` to predict `yr` values over a range of x values given by `xr = np.linspace(0,1,101)`\n",
    "- Plot the data, ground truth function, and regressed model using `plot_model(x,y,xt,yt,xr,yr,title)` with an appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "\n",
    "What differences between the regularized and standard 9th order models fit to `d10` do you notice? How does regularization affect the fitted function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLS with $L_2$ regularization and gradient descent\n",
    "\n",
    "For complex models, the size of $X'X$ can be large, making matrix inversion computationally demanding. Instead, one can use gradient descent to compute $w$. In our notes, we derived the gradient descent approach both for unregularized as well as $L_2$ regularized linear regression. The formula for the gradient descent approach with $L_2$ regularization is:  \n",
    "\n",
    "$ \\frac{\\partial obj}{\\partial w} = X'Xw - X'y + \\lambda \\mathbb{I}_m w$\n",
    "\n",
    "$ w^{new} \\leftarrow w^{cur} - \\alpha \\frac{\\partial obj}{\\partial w}$\n",
    "\n",
    "\n",
    "In this problem, could gradient descent get stuck in a local minimum? Explain why / why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will consider just a single case in the following question:\n",
    "\n",
    "1. Data:  data10.txt, Model: 9th order polynomial.\n",
    "\n",
    "Starting with a weight vector of zeros as the initial guess, and $\\lambda = e^{-10}$, $\\alpha = 0.075$, apply 50000 iterations of\n",
    "gradient descent to find the optimal model parameters. In practice, when you train your own models you will have to determine these parameters yourself!\n",
    "\n",
    "For the trained model:\n",
    "- Print the learned model parameters `w`\n",
    "- Use the model parameters `w` to predict `yr` values over a range of x values given by `xr = np.linspace(0,1,101)`\n",
    "- Plot the data, ground truth function, and regressed model using `plot_model(x,y,xt,yt,xr,yr,title)` with an appropriate title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "\n",
    "Visually compare the result you just obtained to the same 9th order polynomial model with $L_2$ regularization where you solved for `w` directly in the previous section. They should be very similar. Comment on whether gradient descent has converged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
